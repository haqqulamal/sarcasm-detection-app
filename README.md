# ğŸ­ Indonesian Sarcasm Detection - Web Application

A complete web application for detecting sarcasm in Indonesian text using a fine-tuned XLNet model. Built with FastAPI backend and Streamlit frontend, optimized for free deployment on Hugging Face Spaces.

## ğŸ“‹ Overview

This project provides:

- **FastAPI Backend** (`app_api.py`): REST API for model inference
- **Streamlit Frontend** (`app_web.py`): Modern web UI for user interaction
- **Separation of Concerns**: Backend and frontend run independently
- **CPU-Only Inference**: No GPU required
- **Free Deployment**: Compatible with Hugging Face Spaces

## ğŸ—ï¸ Project Structure

```
sarcasm-web/
â”œâ”€â”€ app_api.py              # FastAPI backend (model inference)
â”œâ”€â”€ app_web.py              # Streamlit frontend (user interface)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â””â”€â”€ best_checkpoint/       # Fine-tuned XLNet model folder
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â”œâ”€â”€ special_tokens_map.json
    â”œâ”€â”€ spiece.model
    â””â”€â”€ tokenizer_config.json
```

## ğŸš€ How to Run Locally

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Start the FastAPI Backend

Open a terminal and run:

```bash
python app_api.py
```

The API will be available at `http://localhost:7860`

Health check endpoint: `http://localhost:7860/health`

API documentation: `http://localhost:7860/docs`

### 3. Start the Streamlit Frontend (In a New Terminal)

```bash
streamlit run app_web.py
```

The web app will open at `http://localhost:8501`

## ğŸ“¡ API Endpoints

### Health Check

```
GET /health
Response: {"status": "healthy"}
```

### Root Endpoint

```
GET /
Response: {"status": "ok", "message": "...", "model_loaded": true}
```

### Prediction Endpoint

```
POST /predict
Content-Type: application/json

Request:
{
  "text": "Your Indonesian text here"
}

Response:
{
  "prediction": "Sarcasm" | "Non-Sarcasm",
  "confidence": 0.95
}
```

## ğŸ”§ Configuration

### Model Parameters

- **Model Type**: XLNet (fine-tuned)
- **Max Sequence Length**: 256 tokens
- **Device**: CPU only
- **Inference Mode**: torch.no_grad() (optimized)

### API Configuration

- **Host**: 0.0.0.0
- **Port**: 7860 (default)
- **Workers**: 1 (CPU-optimized)

### Streamlit Configuration

- **Port**: 8501 (default)
- **Max Request Size**: Enforced in frontend (1000 characters)

## ğŸ“Š Model Information

- **Framework**: Hugging Face Transformers
- **Architecture**: XLNet
- **Language**: Indonesian
- **Task**: Sequence Classification (Binary: Sarcasm/Non-Sarcasm)
- **Input**: Text (max 256 tokens)
- **Output**: Prediction + Confidence Score

## âš¡ Performance Optimization

- **CPU-Only**: No CUDA required, runs on any machine
- **No Gradient Computation**: Uses `torch.no_grad()` for faster inference
- **Model Evaluation Mode**: Sets `model.eval()` for inference optimization
- **Batch Disabled**: Single-text inference for simplicity
- **Async Ready**: FastAPI with async support for future scaling

## ğŸ›¡ï¸ Error Handling

- Empty text validation
- Text length validation (max 1000 chars)
- API connection error handling
- Timeout handling (30 seconds)
- Model loading error handling

## ğŸ“ Notes

- Ensure `best_checkpoint` folder is in the same directory as the scripts
- Model loading happens once at API startup
- Each prediction is independent and returns fresh results
- Confidence scores are normalized to 0-1 range

## ğŸ› Troubleshooting

**Issue**: "Cannot connect to API server"

- Solution: Make sure `app_api.py` is running on port 7860

**Issue**: "Model not loaded"

- Solution: Check if `best_checkpoint` folder exists and contains all model files

**Issue**: Out of memory

- Solution: This is optimized for CPU. Consider using smaller batch sizes if deployed on limited hardware.

**Issue**: Slow inference

- Solution: Normal for CPU inference. GPU would be ~10x faster but costs money on cloud platforms.

## ğŸ“„ License

Open source. Feel free to modify and deploy.

## ğŸ¤ Support

For issues or questions, check:

1. Model files are complete in `best_checkpoint`
2. Dependencies installed: `pip install -r requirements.txt`
3. Both services running (API on 7860, Streamlit on 8501)

